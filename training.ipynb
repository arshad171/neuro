{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from models.import_plotting import *\n",
    "from models.types import *\n",
    "from train.utils import *\n",
    "from train.predictors import *\n",
    "\n",
    "USE_GPU = True\n",
    "SCALING_STATS_NAME = \"sts_server1_cpu.json\"\n",
    "NUM_EPOCHS = 10\n",
    "LM_LOSS_WT = 1e-1\n",
    "LM_LOSS_WT_DECAY = 0.9\n",
    "MODEL = \"boot-setrepr-hydra\"\n",
    "# MODEL = \"boot-nn\"\n",
    "USE_BASE_PARAM = True\n",
    "SAVE_POSTFIX = f\"{MODEL}\"\n",
    "BOOTSTRAP = True\n",
    "SAVE_RES = True\n",
    "SAVE_RES_PF = \"\"\n",
    "SAVE_FIGS = False\n",
    "# APPLICATION_CATALOG = ApplicationCatalog.E\n",
    "APPLICATION_CATALOG = ApplicationCatalog.E\n",
    "LOAD_WEIGHTS = False\n",
    "SKIP_VAL = False\n",
    "\n",
    "DS_TRAIN = [\n",
    "    f\"{HOME_PATH}/ds_folder_1\"\n",
    "    f\"{HOME_PATH}/ds_folder_2\"\n",
    "    f\"{HOME_PATH}/ds_folder_3\"\n",
    "]\n",
    "SKIP_DATA_PARSING = False\n",
    "\n",
    "DS_VAL = f\"{HOME_PATH}/folder_val\"\n",
    "\n",
    "\n",
    "SAVE_PATH = f\"{HOME_PATH}/results/models_mp75p95/neuro_t\"\n",
    "\n",
    "LOAD_WEIGHTS_PATH = f\"{HOME_PATH}/server_2/results/models_mp75p95_adap2/neuro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_DATA_PARSING:\n",
    "    ds_runs = get_runs(DS_VAL)\n",
    "\n",
    "    pickle.dump(ds_runs, open(os.path.join(DS_VAL, f\"data_runs.pkl\"), \"wb\"))\n",
    "\n",
    "    dps = get_datapoints(ds_runs, use_gpu=USE_GPU)\n",
    "\n",
    "    pickle.dump(dps, open(os.path.join(DS_VAL, f\"data.pkl\"), \"wb\"))\n",
    "\n",
    "    n_dps = len(dps)\n",
    "\n",
    "    print(n_dps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_DATA_PARSING:\n",
    "    n_dps = 0\n",
    "    for ds_train in DS_TRAIN:\n",
    "        ds_runs_train = get_runs(ds_train)\n",
    "\n",
    "        print(len(ds_runs_train))\n",
    "\n",
    "        pickle.dump(ds_runs_train, open(os.path.join(ds_train, f\"data_runs.pkl\"), \"wb\"))\n",
    "\n",
    "        dps = get_datapoints(ds_runs_train, use_gpu=USE_GPU)\n",
    "\n",
    "        pickle.dump(dps, open(os.path.join(ds_train, f\"data.pkl\"), \"wb\"))\n",
    "\n",
    "        n_dps += len(dps)\n",
    "\n",
    "        print(n_dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_runs_train = []\n",
    "for ds_train in DS_TRAIN:\n",
    "    ds = pickle.load(open(os.path.join(ds_train, f\"data_runs.pkl\"), \"rb\"))\n",
    "    print(len(ds))\n",
    "\n",
    "    ds_runs_train.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_runs_val = pickle.load(open(os.path.join(DS_VAL, f\"data_runs.pkl\"), \"rb\"))\n",
    "print(len(ds_runs_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datapoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints_train = []\n",
    "for ds_train in DS_TRAIN:\n",
    "    dps = pickle.load(\n",
    "        open(os.path.join(ds_train, f\"data.pkl.nb\" if not BOOTSTRAP else \"data.pkl\"), \"rb\")\n",
    "    )\n",
    "    datapoints_train.append(dps)\n",
    "    print(len(datapoints_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints_val = pickle.load(open(os.path.join(DS_VAL, \"data.pkl\"), \"rb\"))\n",
    "print(len(datapoints_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(ds_folder, split=False, use_pairwise=False, bootstrap=False, flag=False):\n",
    "    if not isinstance(ds_folder, list):\n",
    "        ds_folder = [ds_folder]\n",
    "\n",
    "    seed = 0\n",
    "    th.manual_seed(seed)\n",
    "\n",
    "    ds_list = []\n",
    "    for folder in ds_folder:\n",
    "        match MODEL:\n",
    "            case \"boot-setrepr\":\n",
    "                ds = CustomDatasetSetRepr(\n",
    "                    path=os.path.join(folder, f\"data.pkl.nb\" if not bootstrap else \"data.pkl\")\n",
    "                )\n",
    "            case \"boot-setrepr-hydra\":\n",
    "                ds = CustomDatasetSetReprHydra(\n",
    "                    path=os.path.join(folder, f\"data.pkl.nb\" if not bootstrap else \"data.pkl\"),\n",
    "                    use_base_param=USE_BASE_PARAM,\n",
    "                    scaling_stats_name=SCALING_STATS_NAME,\n",
    "                    use_gpu=USE_GPU,\n",
    "                )\n",
    "            case \"boot-nn\":\n",
    "                ds = CustomDataset(path=os.path.join(folder, \"data.pkl\"))\n",
    "            case _:\n",
    "                print(\"Incorrect model name\")\n",
    "\n",
    "        ds_list.append(ds)\n",
    "\n",
    "    ds = th.utils.data.ConcatDataset(ds_list)\n",
    "\n",
    "    if flag:\n",
    "        total_size = len(ds)\n",
    "        split_size = int(0.4 * total_size)  # 30% of the dataset\n",
    "        _, ds = random_split(ds, [total_size - split_size, split_size])\n",
    "\n",
    "    if split:\n",
    "        frac = 0.2\n",
    "        train_ds, val_ds = random_split(ds, [len(ds) - int(frac * len(ds)), int(frac * len(ds))])\n",
    "        train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "        val_dl = DataLoader(val_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "        return train_ds, train_dl, val_ds, val_dl\n",
    "    else:\n",
    "        return ds, DataLoader(ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds, train_dl, val_ds, val_dl = get_ds(DS_TRAIN, split=True, use_pairwise=DS_USE_PAIRWISE)\n",
    "\n",
    "train_ds, train_dl = get_ds(\n",
    "    DS_TRAIN,\n",
    "    split=False,\n",
    "    bootstrap=BOOTSTRAP,\n",
    "    flag=True and BOOTSTRAP,\n",
    ")\n",
    "val_ds, val_dl = get_ds(DS_VAL, split=False, use_pairwise=False, bootstrap=True)\n",
    "\n",
    "print(train_ds[0], val_ds[0])\n",
    "\n",
    "# if SAVE_RES:\n",
    "#     json.dump(\n",
    "#         {\"train_indices\": train_ds.indices, \"val_indices\": val_ds.indices},\n",
    "#         open(os.path.join(DS_FOLDER, \"data_indices.json\"), \"w\"),\n",
    "#         indent=4,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Normalizing constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">❗❗❗</span> Comment out scaling in the dataset before running this section (manual step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_max_exec = {a: [0 for _ in range(len(APP_INFERENCE_TARGETS))] for a in APPLICATIONS_E}\n",
    "# app_min_exec = {a: [100 for _ in range(len(APP_INFERENCE_TARGETS))] for a in APPLICATIONS_E}\n",
    "# app_exec = {a: [[] for _ in range(len(APP_INFERENCE_TARGETS))] for a in APPLICATIONS_E}\n",
    "\n",
    "# for ix in range(3):\n",
    "#     for d in train_ds:\n",
    "#         a = d[0][\"x_features\"][0]\n",
    "#         a = APPLICATIONS_E[int(a)]\n",
    "#         ts = d[1][0][ix]\n",
    "\n",
    "\n",
    "#         app_exec[a][ix].append(ts.item())\n",
    "#         if ts > app_max_exec[a][ix]:\n",
    "#             app_max_exec[a][ix] = ts.item()\n",
    "\n",
    "#         if ts < app_min_exec[a][ix] and ts != 0:\n",
    "#             app_min_exec[a][ix] = ts.item()\n",
    "\n",
    "# print(app_max_exec)\n",
    "# print(app_min_exec)\n",
    "\n",
    "# for (k1, v1), (k2, v2) in zip(app_max_exec.items(), app_min_exec.items()):\n",
    "#     print(k1, v1[2], v2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APP_EXEC_MAX = {'tclf-gcn': [13.368155479431152, 17.26291275024414, 22.0528507232666], 'tclf-rf': [0.011183061636984348, 0.01432347297668457, 0.0415102019906044], 'tstr-lstm': [0.525345504283905, 0.6639305353164673, 0.7699626684188843], 'kalman-gru': [0.012417145073413849, 0.003949642181396484, 0.08132541179656982], 'iclf-mnet': [102.6425552368164, 105.85325622558594, 108.42182922363281], 'text-bert': [41.83883285522461, 46.11003112792969, 56.48748016357422], 'iclf-efnet': [120.0, 118.8111572265625, 120.0], 'text-tbert': [8.286052703857422, 9.772259712219238, 13.742752075195312], 'iclf-mvit': [81.0706787109375, 88.86546325683594, 95.1012954711914]}\n",
    "# APP_EXEC_MIN = {'tclf-gcn': [0.04118216037750244, 0.04308032989501953, 0.049125004559755325], 'tclf-rf': [0.005355142056941986, 0.005410194396972656, 0.005952763371169567], 'tstr-lstm': [0.13362009823322296, 0.13778185844421387, 0.1463068723678589], 'kalman-gru': [0.0021471274085342884, 0.0022356510162353516, 0.0026605844032019377], 'iclf-mnet': [0.07093993574380875, 0.06460070610046387, 0.08770449459552765], 'text-bert': [0.16787423193454742, 0.1311308741569519, 0.2003573179244995], 'iclf-efnet': [0.5715664625167847, 0.003205239772796631, 0.44005441665649414], 'text-tbert': [0.011626155115664005, 0.012388646602630615, 0.01636815071105957], 'iclf-mvit': [0.05962417274713516, 0.044938087463378906, 0.06649579852819443]}\n",
    "# T_IX = 1\n",
    "\n",
    "# app_max_exec = {a: [] for a in APPLICATIONS_E}\n",
    "\n",
    "# for d in train_ds:\n",
    "#     a = d[0][\"x_features\"][0]\n",
    "#     a = APPLICATIONS_E[int(a)]\n",
    "#     ts = d[1][0][T_IX]\n",
    "\n",
    "#     app_max_exec[a].append(ts.item())\n",
    "\n",
    "# for a, d in app_max_exec.items():\n",
    "#     d = np.array(d)\n",
    "\n",
    "#     d = d / APP_EXEC_MAX[a][T_IX]\n",
    "\n",
    "#     plt.hist(d, label=a, bins=20)\n",
    "#     # plt.hist((d - np.mean(d)) / np.std(d), label=a)\n",
    "#     # plt.hist((d)/ (np.max(d)), label=a)\n",
    "#     # plt.hist((d - d.min())/ (np.max(d) - d.min()), label=a)\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Save/Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1 = ServicePredictorNNSetReprHydra(embedding_type=EmbeddingTypes.USE_EMBEDDING_LAYER, application_catalog=ApplicationCatalog.V1)\n",
    "# s1.save_weights(path=DS_FOLDER)\n",
    "# print(\"saved\")\n",
    "# print(s1.embedding.weight.data)\n",
    "\n",
    "# s2 = ServicePredictorNNSetReprHydra(embedding_type=EmbeddingTypes.USE_EMBEDDING_LAYER, application_catalog=ApplicationCatalog.V2)\n",
    "# s2.load_weights(path=DS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match MODEL:\n",
    "    case \"boot-setrepr\":\n",
    "        service_predictor = ServicePredictorNNSetRepr(\n",
    "            embedding_type=EmbeddingTypes.USE_EMBEDDING_LAYER\n",
    "        )\n",
    "    case \"boot-setrepr-hydra\":\n",
    "        service_predictor = ServicePredictorNNSetReprHydra(\n",
    "            use_base_param=USE_BASE_PARAM,\n",
    "            embedding_type=EmbeddingTypes.USE_EMBEDDING_LAYER,\n",
    "            application_catalog=APPLICATION_CATALOG,\n",
    "            use_gpu=USE_GPU,\n",
    "        )\n",
    "        if LOAD_WEIGHTS:\n",
    "            service_predictor.load_weights(path=LOAD_WEIGHTS_PATH, expand_embedd=True)\n",
    "\n",
    "        # if LOAD_WEIGHTS and APPLICATION_CATALOG in [\n",
    "        #     ApplicationCatalog.V21,\n",
    "        #     ApplicationCatalog.V22,\n",
    "        # ]:\n",
    "        #     service_predictor.load_weights(path=DS_FOLDER_V1, expand_embedd=True)\n",
    "    case \"boot-nn\":\n",
    "        service_predictor = ServicePredictorNN()\n",
    "    case _:\n",
    "        print(\"Incorrect model name\")\n",
    "\n",
    "\n",
    "# loading\n",
    "# service_predictor.load_state_dict(th.load(os.path.join(DS_FOLDER, f\"saved_model_{SAVE_POSTFIX}\"), weights_only=True))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(service_predictor.parameters())\n",
    "\n",
    "num_params = sum(p.numel() for p in service_predictor.parameters())\n",
    "print(\"num. trainable params\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == \"boot-setrepr-hydra\":\n",
    "    sts_val_losses = []\n",
    "    sts_train_losses = []\n",
    "\n",
    "    lm_val_losses = []\n",
    "    lm_train_losses = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        lm_loss_wt = LM_LOSS_WT * 0.9**epoch\n",
    "        print(f\"{epoch=}\")\n",
    "        sts_val_loss = 0\n",
    "        lm_val_loss = 0\n",
    "\n",
    "        if epoch % 5 == 0 and not SKIP_VAL:\n",
    "            with th.no_grad():\n",
    "                for data in val_dl:\n",
    "                    x, (y_sts, y_lm) = data\n",
    "\n",
    "                    out_sts, out_lm = service_predictor(x)\n",
    "\n",
    "                    loss_sts = loss_fn(out_sts, y_sts)\n",
    "                    loss_lm = loss_fn(out_lm, y_lm)\n",
    "                    loss_lm *= lm_loss_wt\n",
    "\n",
    "                    sts_val_loss += loss_sts.item()\n",
    "                    lm_val_loss += loss_lm.item()\n",
    "\n",
    "                sts_val_loss /= len(val_dl)\n",
    "                lm_val_loss /= len(val_dl)\n",
    "\n",
    "                sts_val_losses.append(sts_val_loss)\n",
    "                lm_val_losses.append(lm_val_loss)\n",
    "\n",
    "            sts_train_loss = 0\n",
    "            lm_train_loss = 0\n",
    "            with th.no_grad():\n",
    "                for data in train_dl:\n",
    "                    x, (y_sts, y_lm) = data\n",
    "\n",
    "                    out_sts, out_lm = service_predictor(x)\n",
    "                    loss_sts = loss_fn(out_sts, y_sts)\n",
    "                    loss_lm = loss_fn(out_lm, y_lm)\n",
    "                    loss_lm *= lm_loss_wt\n",
    "\n",
    "                    sts_train_loss += loss_sts.item()\n",
    "                    lm_train_loss += loss_lm.item()\n",
    "\n",
    "                sts_train_loss /= len(train_dl)\n",
    "                lm_train_loss /= len(train_dl)\n",
    "\n",
    "                sts_train_losses.append(sts_train_loss)\n",
    "                lm_train_losses.append(lm_train_loss)\n",
    "\n",
    "        for data in tqdm(train_dl):\n",
    "            x, (y_sts, y_lm) = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out_sts, out_lm = service_predictor(x)\n",
    "\n",
    "            loss_sts = loss_fn(out_sts, y_sts)\n",
    "            loss_lm = loss_fn(out_lm, y_lm)\n",
    "            loss_lm *= lm_loss_wt\n",
    "\n",
    "            loss = loss_sts + loss_lm\n",
    "\n",
    "            loss.backward()\n",
    "            th.nn.utils.clip_grad_norm_(service_predictor.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "    sts_train_losses = np.array(sts_train_losses)\n",
    "    sts_val_losses = np.array(sts_val_losses)\n",
    "    lm_train_losses = np.array(lm_train_losses)\n",
    "    lm_val_losses = np.array(lm_val_losses)\n",
    "\n",
    "    # if APPLICATION_CATALOG == ApplicationCatalog.V1:\n",
    "    #     service_predictor.save_weights(path=DS_FOLDER_V1)\n",
    "    if SAVE_RES:\n",
    "        service_predictor.save_weights(path=SAVE_PATH)\n",
    "    # th.save(service_predictor.state_dict(), os.path.join(DS_FOLDER, f\"saved_model_{SAVE_POSTFIX}\"))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(list(range(1, NUM_EPOCHS + 1, 5)), sts_train_losses, \"red\", label=\"Training\")\n",
    "    if len(sts_val_losses) > 0:\n",
    "        plt.plot(list(range(1, NUM_EPOCHS + 1, 5)), sts_val_losses, \"blue\", label=\"Validation\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel(r\"\\textbf{Epochs}\")\n",
    "    plt.ylabel(r\"$\\log$ \\textbf{MSE}\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    if SAVE_FIGS:\n",
    "        plt.savefig(os.path.join(FIGS_PATH, f\"training_progress_{SAVE_POSTFIX}.pdf\"))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(list(range(1, NUM_EPOCHS + 1, 5)), lm_train_losses, \"red\", label=\"Training\")\n",
    "    if len(lm_val_losses) > 0:\n",
    "        plt.plot(list(range(1, NUM_EPOCHS + 1, 5)), lm_val_losses, \"blue\", label=\"Validation\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel(r\"\\textbf{Epochs}\")\n",
    "    plt.ylabel(r\"$\\log$ \\textbf{MSE}\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    if SAVE_FIGS:\n",
    "        plt.savefig(os.path.join(FIGS_PATH, f\"training_progress_{SAVE_POSTFIX}.pdf\"))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "training_progress = {\n",
    "    \"train_losses\": {\"sts\": sts_train_losses.tolist(), \"lm\": lm_train_losses.tolist()},\n",
    "    \"val_losses\": {\"sts\": sts_val_losses.tolist(), \"lm\": lm_val_losses.tolist()},\n",
    "}\n",
    "\n",
    "if SAVE_RES:\n",
    "    json.dump(\n",
    "        training_progress,\n",
    "        open(os.path.join(SAVE_PATH, f\"training_progress_{SAVE_POSTFIX}.json\"), \"w\"),\n",
    "        indent=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_predictor.load_weights(path=SAVE_PATH, expand_embedd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = json.load(open(\"models/sts_server1_gpu.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_dl, target_ix=0):\n",
    "    errors = []\n",
    "    errors_a = []\n",
    "    if MODEL == \"boot-setrepr-hydra\":\n",
    "        with th.no_grad():\n",
    "            for batch in tqdm(val_dl):\n",
    "                x, (y_sts, y_lm) = batch\n",
    "                y_pred_sts, y_pred_lm = service_predictor(x)\n",
    "                y_pred_sts = y_pred_sts.detach()\n",
    "\n",
    "                y_sts = y_sts[:, target_ix]\n",
    "                y_pred_sts = y_pred_sts[:, target_ix]\n",
    "\n",
    "                mask = y_sts != 0\n",
    "                y_sts = y_sts[mask]\n",
    "                y_pred_sts = y_pred_sts[mask]\n",
    "\n",
    "\n",
    "                errors.extend(((y_pred_sts - y_sts) / y_sts)[:].numpy().tolist())\n",
    "\n",
    "\n",
    "    if MODEL == \"boot-setrepr\" or MODEL == \"boot-nn\":\n",
    "        with th.no_grad():\n",
    "            for batch in tqdm(val_dl):\n",
    "                x, y = batch\n",
    "                y_pred = service_predictor(x)\n",
    "\n",
    "                errors.extend(((y_pred - y) / y)[:, 0].numpy().tolist())\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "def plot_errors(errors):\n",
    "    plt.figure()\n",
    "    x_vals1, kde_vals1 = get_kde(errors)\n",
    "    plt.fill_between(x_vals1, kde_vals1, color=\"crimson\")\n",
    "    # plt.hist(errors, bins=100, density=True)\n",
    "\n",
    "    return x_vals1, kde_vals1\n",
    "\n",
    "\n",
    "target_ix = 1\n",
    "errors = evaluate(val_dl, target_ix=target_ix)\n",
    "errors = np.array(errors)\n",
    "errors = errors[np.isfinite(errors)]\n",
    "errors = errors.tolist()\n",
    "x_vals, kde_vals = plot_errors(errors)\n",
    "\n",
    "if SAVE_RES:\n",
    "    json.dump(\n",
    "        {\"errors\": errors},\n",
    "        open(\n",
    "            os.path.join(SAVE_PATH, f\"errors_{SAVE_POSTFIX}{SAVE_RES_PF}_t{target_ix}.json\"), \"w\"\n",
    "        ),\n",
    "        indent=4,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pa_res_alloc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
